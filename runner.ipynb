{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to HW4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from einops import rearrange\n",
    "from stable_audio_tools import get_pretrained_model\n",
    "import IPython.display as ipd\n",
    "from tqdm.auto import trange, tqdm\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond_and_sampler_setup, generate_diffusion_cond_decode\n",
    "import IPython.display as ipd\n",
    "from homework4_stub import simple_sample, generate_inpainting_mask, simple_sample_inpaint, simple_sample_variable_inpaint, simple_sample_style_transfer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Download model\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-1.0\")\n",
    "SAMPLE_RATE = model_config[\"sample_rate\"]\n",
    "SAMPLE_SIZE = model_config[\"sample_size\"]\n",
    "SEED = 456\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Simple Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should implement the to_d and simple_sample functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt=\"128 BPM electronic drum loop\", steps=100, cfg_scale=7, return_latents=False):\n",
    "\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 47\n",
    "    }]\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps, # number of steps, more = better quality\n",
    "        cfg_scale=cfg_scale, # Classifier-Free Guidance Scale, higher = better text relevance / quality but less diversity\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE, # number of audio samples to generate, DON'T CHANGE\n",
    "        device=device, # cuda device\n",
    "        seed=SEED # random seed, DON'T CHANGE\n",
    "    )\n",
    "\n",
    "    # Sample\n",
    "    samples = simple_sample(denoiser, x_T, sigmas, extra_args=extra_args)\n",
    "\n",
    "    if return_latents:\n",
    "        return samples\n",
    "\n",
    "    # Decode\n",
    "    audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        samples\n",
    "    )\n",
    "    return audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    \"128 BPM electronic drum loop\",\n",
    "    \"176 bpm drum-n-bass break\",\n",
    "    \"water\",\n",
    "    \"a sports car passing by\",\n",
    "    \"vibrant synth arpeggio, C minor\",\n",
    "    \"glitchy sub bass patch, I made it in Serum\"\n",
    "]\n",
    "\n",
    "outputs = []\n",
    "for prompt in prompt_list:\n",
    "    outputs.append(generate(prompt, return_latents=True).cpu().numpy())\n",
    "\n",
    "answers['Q1'] = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - Inpainting Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AND ENCODE REFERENCE AUDIO\n",
    "def load_and_encode_audio(path, model):\n",
    "    audio, sr = torchaudio.load(path)\n",
    "    # resample to SAMPLE_RATE\n",
    "    resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
    "    sr = SAMPLE_RATE\n",
    "    audio = resampler(audio)\n",
    "    # peak normalize\n",
    "    audio = audio / audio.abs().max()\n",
    "\n",
    "    # trim to SAMPLE_SIZE if longer, pad with repetition if shorter\n",
    "    if audio.shape[1] < SAMPLE_SIZE:\n",
    "        while audio.shape[1] < SAMPLE_SIZE:\n",
    "            audio = torch.cat((audio, audio), dim=1)\n",
    "\n",
    "    audio = audio[:, :SAMPLE_SIZE][None].to(device)\n",
    "\n",
    "    reference = model.pretransform.encode(audio)\n",
    "    return reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list = []\n",
    "for mask_ranges in [(0, 5), (0, 10), (0, 20), (20, 30), (30, 40), (20, 47)]:\n",
    "    mask = generate_inpainting_mask(outputs[0], *mask_ranges)\n",
    "    mask_list.append(mask.cpu().numpy())\n",
    "\n",
    "answers['Q2'] = mask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3  - Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint(prompt=\"128 BPM house drum loop\", steps=100, cfg_scale=7, reference=None, mask_start_s=20, mask_end_s=30, return_latents=False):\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 47\n",
    "    }]\n",
    "    # Set up inpainting mask\n",
    "    mask = generate_inpainting_mask(reference, mask_start_s, mask_end_s)\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    # Sample\n",
    "    inp_samples = simple_sample_inpaint(denoiser, x_T, sigmas, reference, mask, extra_args=extra_args)\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "\n",
    "    # decode and play\n",
    "    inpainted_audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        inp_samples\n",
    "    )\n",
    "    return inpainted_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ranges = [(0, 5), (0, 10), (0, 20), (20, 30), (15, 30), (20, 47)]\n",
    "prompt_list = [\n",
    "    \"128 BPM electronic drum loop\",\n",
    "    \"176 bpm drum-n-bass break\",\n",
    "    \"water\",\n",
    "    \"a sports car passing by\",\n",
    "    \"vibrant synth arpeggio, C minor\",\n",
    "    \"glitchy sub bass patch, I made it in Serum\"\n",
    "]\n",
    "reference_list = [\n",
    "    \"references/0.wav\",\n",
    "    \"references/1.wav\",\n",
    "    \"references/2.wav\",\n",
    "    \"references/3.wav\",\n",
    "    \"references/4.wav\",\n",
    "    \"references/5.wav\"\n",
    "]\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for i, reference_path in enumerate(reference_list):\n",
    "    reference = load_and_encode_audio(reference_path, model)\n",
    "    outputs.append(inpaint(prompt=prompt_list[i], reference=reference, mask_start_s=mask_ranges[i][0], mask_end_s=mask_ranges[i][1], return_latents=True).cpu().numpy())\n",
    "\n",
    "answers['Q3'] = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Painting with Starting and Stopping Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_inpaint(prompt=\"128 BPM house drum loop\", steps=100, cfg_scale=7, reference=None, mask_start_s=20, mask_end_s=30, paint_start=None, paint_end=None, return_latents=False):\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 47\n",
    "    }]\n",
    "    # Set up inpainting mask\n",
    "    mask = generate_inpainting_mask(reference, mask_start_s, mask_end_s)\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    # Sample\n",
    "    inp_samples = simple_sample_variable_inpaint(denoiser, x_T, sigmas, reference, mask, extra_args=extra_args, paint_start=paint_start, paint_end=paint_end)\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "\n",
    "    # decode and play\n",
    "    inpainted_audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        inp_samples\n",
    "    )\n",
    "    return inpainted_audio\n",
    "# inpainted_audio = variable_inpaint(reference=reference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ranges = [(0, 5), (0, 10), (0, 20), (20, 30), (15, 30), (20, 47)]\n",
    "paint_ranges = [(0, 50), (0, 40), (0, 30), (0, 70), (20, 70), (30, 100)]\n",
    "prompt_list = [\n",
    "    \"128 BPM electronic drum loop\",\n",
    "    \"176 bpm drum-n-bass break\",\n",
    "    \"water\",\n",
    "    \"a sports car passing by\",\n",
    "    \"vibrant synth arpeggio, C minor\",\n",
    "    \"glitchy sub bass patch, I made it in Serum\"\n",
    "]\n",
    "reference_list = [\n",
    "    \"references/0.wav\",\n",
    "    \"references/1.wav\",\n",
    "    \"references/2.wav\",\n",
    "    \"references/3.wav\",\n",
    "    \"references/4.wav\",\n",
    "    \"references/5.wav\"\n",
    "]\n",
    "outputs = []\n",
    "for i, reference_path in enumerate(reference_list):\n",
    "    reference = load_and_encode_audio(reference_path, model)\n",
    "    outputs.append(variable_inpaint(prompt=prompt_list[i], reference=reference, mask_start_s=mask_ranges[i][0], mask_end_s=mask_ranges[i][1], paint_start=paint_ranges[i][0], paint_end=paint_ranges[i][1], return_latents=True).cpu().numpy())\n",
    "answers['Q4'] = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(prompt=\"128 BPM house drum loop\", steps=100, cfg_scale=7, reference=None, transfer_strength=0, return_latents=False):\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 47\n",
    "    }]\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    # Sample\n",
    "    inp_samples = simple_sample_style_transfer(denoiser, sigmas, reference, extra_args=extra_args, transfer_strength=transfer_strength)\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "    \n",
    "    # decode and play\n",
    "    inpainted_audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        inp_samples\n",
    "    )\n",
    "    return inpainted_audio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strengths = [0.1, 0.3, 0.4, 0.6, 0.45, 0.9]\n",
    "prompt_list = [\n",
    "    \"glitchy sub bass patch, I made it in Serum\",\n",
    "    \"128 BPM electronic drum loop\",\n",
    "    \"176 bpm drum-n-bass break\",\n",
    "    \"water\",\n",
    "    \"a sports car passing by\",\n",
    "    \"vibrant synth arpeggio, C minor\",\n",
    "]\n",
    "reference_list = [\n",
    "    \"references/0.wav\",\n",
    "    \"references/1.wav\",\n",
    "    \"references/2.wav\",\n",
    "    \"references/3.wav\",\n",
    "    \"references/4.wav\",\n",
    "    \"references/5.wav\"\n",
    "]\n",
    "outputs = []\n",
    "for i, reference_path in enumerate(reference_list):\n",
    "    reference = load_and_encode_audio(reference_path, model)\n",
    "    outputs.append(style_transfer(prompt=prompt_list[i], reference=reference, transfer_strength=strengths[i], return_latents=True).cpu().numpy())\n",
    "\n",
    "answers['Q5'] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"homework4.pkl\", \"wb\") as f:\n",
    "    pickle.dump(answers, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
