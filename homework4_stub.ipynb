{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to HW4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load in our model, and initialize our global variables of SAMPLE_RATE (i.e. the samples per second of the audio, in this case 44100), SAMPLE_SIZE (the *number* of audio samples we generate with the model, approximately 47.55*44100), and SEED (controls randomness, DO NOT CHANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from einops import rearrange\n",
    "from stable_audio_tools import get_pretrained_model\n",
    "import IPython.display as ipd\n",
    "from tqdm.auto import trange, tqdm\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond_and_sampler_setup, generate_diffusion_cond_decode\n",
    "import IPython.display as ipd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Download model\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-1.0\")\n",
    "SAMPLE_RATE = model_config[\"sample_rate\"]\n",
    "SAMPLE_SIZE = model_config[\"sample_size\"]\n",
    "SEED = 456\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Simple Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should implement the to_d and simple_sample functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_d(x, sigma, denoised):\n",
    "    return #TODO\n",
    "\n",
    "@torch.no_grad()\n",
    "def simple_sample(model, x, sigmas, extra_args=None):\n",
    "    extra_args = {} if extra_args is None else extra_args\n",
    "    s_in = x.new_ones([x.shape[0]])\n",
    "    for i in trange(len(sigmas) - 1):\n",
    "        #TODO\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given your code, you can now run it using this below block. Feel free to play around with the prompt in the conditioning list, the number of steps, and cfg_scale to explore unique outputs. This can help you test your code, as if it sounds bad, you're probably doing something wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt=\"128 BPM electronic drum loop\", steps=100, cfg_scale=7, return_latents=False):\n",
    "\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 47\n",
    "    }]\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps, # number of steps, more = better quality\n",
    "        cfg_scale=cfg_scale, # Classifier-Free Guidance Scale, higher = better text relevance / quality but less diversity\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE, # number of audio samples to generate, DON'T CHANGE\n",
    "        device=device, # cuda device\n",
    "        seed=SEED # random seed, DON'T CHANGE\n",
    "    )\n",
    "\n",
    "    # Sample\n",
    "    samples = simple_sample(denoiser, x_T, sigmas, extra_args=extra_args)\n",
    "\n",
    "    if return_latents:\n",
    "        return samples\n",
    "\n",
    "    # Decode\n",
    "    audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        samples\n",
    "    )\n",
    "    return audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test your function, you can use the following code\n",
    "\n",
    "audio = generate()\n",
    "ipd.display(ipd.Audio(audio.cpu().numpy(), rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - Inpainting Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AND ENCODE REFERENCE AUDIO\n",
    "def load_and_encode_audio(path, model):\n",
    "    audio, sr = torchaudio.load(path)\n",
    "    # resample to SAMPLE_RATE\n",
    "    resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
    "    sr = SAMPLE_RATE\n",
    "    audio = resampler(audio)\n",
    "    # peak normalize\n",
    "    audio = audio / audio.abs().max()\n",
    "\n",
    "    # trim to SAMPLE_SIZE if longer, pad with repetition if shorter\n",
    "    if audio.shape[1] < SAMPLE_SIZE:\n",
    "        while audio.shape[1] < SAMPLE_SIZE:\n",
    "            audio = torch.cat((audio, audio), dim=1)\n",
    "\n",
    "    audio = audio[:, :SAMPLE_SIZE][None].to(device)\n",
    "\n",
    "    reference = model.pretransform.encode(audio)\n",
    "    return reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inpainting_mask(reference, mask_start_s, mask_end_s):\n",
    "    # TODO\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3  - Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simple_sample_inpaint(model, x, sigmas, reference, mask, extra_args=None):\n",
    "    extra_args = {} if extra_args is None else extra_args\n",
    "    s_in = x.new_ones([x.shape[0]])\n",
    "    for i in trange(len(sigmas) - 1):\n",
    "        # TODO\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint(prompt=\"128 BPM house drum loop\", steps=100, cfg_scale=7, reference=None, mask_start_s=20, mask_end_s=30, return_latents=False):\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 47\n",
    "    }]\n",
    "    # Set up inpainting mask\n",
    "    mask = generate_inpainting_mask(reference, mask_start_s, mask_end_s)\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    # Sample\n",
    "    inp_samples = simple_sample_inpaint(denoiser, x_T, sigmas, reference, mask, extra_args=extra_args)\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "\n",
    "    # decode and play\n",
    "    inpainted_audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        inp_samples\n",
    "    )\n",
    "    return inpainted_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to test your function, you can use the following code\n",
    "# load reference audio\n",
    "reference = load_and_encode_audio(\"references/0.wav\", model)\n",
    "\n",
    "inpainted_audio = inpaint(reference=reference)\n",
    "ipd.display(ipd.Audio(inpainted_audio.cpu().numpy(), rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Painting with Starting and Stopping Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simple_sample_variable_inpaint(model, x, sigmas, reference, mask, extra_args=None, paint_start=None, paint_end=None):\n",
    "    if paint_start is None:\n",
    "        paint_start = 0\n",
    "    if paint_end is None:\n",
    "        paint_end = len(sigmas) - 1\n",
    "    extra_args = {} if extra_args is None else extra_args\n",
    "    s_in = x.new_ones([x.shape[0]])\n",
    "    for i in trange(len(sigmas) - 1):\n",
    "        # TODO\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_inpaint(prompt=\"128 BPM house drum loop\", steps=100, cfg_scale=7, reference=None, mask_start_s=20, mask_end_s=30, paint_start=None, paint_end=None, return_latents=False):\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 47\n",
    "    }]\n",
    "    # Set up inpainting mask\n",
    "    mask = generate_inpainting_mask(reference, mask_start_s, mask_end_s)\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    # Sample\n",
    "    inp_samples = simple_sample_variable_inpaint(denoiser, x_T, sigmas, reference, mask, extra_args=extra_args, paint_start=paint_start, paint_end=paint_end)\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "\n",
    "    # decode and play\n",
    "    inpainted_audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        inp_samples\n",
    "    )\n",
    "    return inpainted_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test your function, you can use the following code\n",
    "# load reference audio\n",
    "reference = load_and_encode_audio(\"references/0.wav\", model)\n",
    "inpainted_audio = variable_inpaint(reference=reference)\n",
    "\n",
    "ipd.display(ipd.Audio(inpainted_audio.cpu().numpy(), rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sample_style_transfer(model, sigmas, reference, extra_args=None, transfer_strength=0):\n",
    "    # TODO\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(prompt=\"128 BPM house drum loop\", steps=100, cfg_scale=7, reference=None, transfer_strength=0, return_latents=False):\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 47\n",
    "    }]\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    # Sample\n",
    "    inp_samples = simple_sample_style_transfer(denoiser, sigmas, reference, extra_args=extra_args, transfer_strength=transfer_strength)\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "    \n",
    "    # decode and play\n",
    "    inpainted_audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        inp_samples\n",
    "    )\n",
    "    return inpainted_audio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test your function, you can use the following code\n",
    "# load reference audio\n",
    "reference = load_and_encode_audio(\"references/0.wav\", model)\n",
    "st_audio = style_transfer(reference=reference)\n",
    "\n",
    "ipd.display(ipd.Audio(st_audio.cpu().numpy(), rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
