{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to HW4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load in our model, and initialize our global variables of SAMPLE_RATE (i.e. the samples per second of the audio, in this case 44100), SAMPLE_SIZE (the *number* of audio samples we generate with the model, approximately 47.55*44100/8), and SEED (controls randomness, DO NOT CHANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from einops import rearrange\n",
    "from stable_audio_tools import get_pretrained_model\n",
    "import IPython.display as ipd\n",
    "from tqdm.auto import trange, tqdm\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond_and_sampler_setup, generate_diffusion_cond_decode\n",
    "import IPython.display as ipd\n",
    "import gc\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.set_default_tensor_type(torch.Tensor)\n",
    "\n",
    "\n",
    "# Download model\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-1.0\")\n",
    "SAMPLE_RATE = model_config[\"sample_rate\"]\n",
    "SAMPLE_SIZE = model_config[\"sample_size\"] // 8\n",
    "SEED = 456\n",
    "\n",
    "# set seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = model.half()\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using collab, uncomment out the following lines\n",
    "# from google.colab import drive\n",
    "# drive.mount('[/content/drive]')\n",
    "# cd /content/drive/MyDrive/[path to your folder]\n",
    "# pip install -e .\n",
    "# pip install numpy==1.26.4\n",
    "# pip install protobuf==3.20.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Simple Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should implement the to_d and simple_sample functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_d(x, sigma, denoised):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def simple_sample(model, x, sigmas, extra_args=None):\n",
    "    extra_args = {} if extra_args is None else extra_args\n",
    "    s_in = x.new_ones([x.shape[0]])\n",
    "    for i in trange(len(sigmas) - 1):\n",
    "        # TODO: add extra_args\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt=\"128 BPM electronic drum loop\", steps=50, cfg_scale=7, return_latents=False, x_start=None):\n",
    "\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 5\n",
    "    }]\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps, # number of steps, more = better quality\n",
    "        cfg_scale=cfg_scale, # Classifier-Free Guidance Scale, higher = better text relevance / quality but less diversity\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE, # number of audio samples to generate, DON'T CHANGE\n",
    "        device=device, # cuda device\n",
    "        seed=SEED # random seed, DON'T CHANGE\n",
    "    )\n",
    "\n",
    "    if x_start is not None:\n",
    "        x_T = x_start\n",
    "\n",
    "    # Sample\n",
    "    samples = simple_sample(denoiser, x_T, sigmas, extra_args=extra_args)\n",
    "    del x_T\n",
    "    del sigmas\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if return_latents:\n",
    "        return samples\n",
    "\n",
    "    # Decode\n",
    "    audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        samples\n",
    "    ).cpu()\n",
    "    return audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test your function, we provide some exampple latents to compare against\n",
    "# the MSE between your latents and the reference latents should be low\n",
    "for ix, prompt in enumerate([\"lo-fi jazz piano in a rainy cafe\", \"deep ambient wash with ocean sounds\"]):\n",
    "    # load reference from testing_files\n",
    "    ref = torch.load(f\"testing_files/q1_{ix}.pt\").to(device)\n",
    "\n",
    "    # load x_T\n",
    "    x_T = torch.load(f\"x_T.pt\").to(device)\n",
    "\n",
    "    latents = generate(prompt=prompt, steps=50, cfg_scale=7, return_latents=True, x_start=x_T)\n",
    "    # compare latents\n",
    "    print(f\"Latent MSE: {torch.nn.functional.mse_loss(ref, latents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those running with a GPU, you can also generate audio samples\n",
    "# this is unpractical for CPU, but you can try it if you want (we DO NOT recommend it)\n",
    "# for ix, prompt in enumerate([\"lo-fi jazz piano in a rainy cafe\", \"deep ambient wash with ocean sounds\"]):\n",
    "#     # load reference from testing_files\n",
    "#     audio = generate(prompt=\"128 BPM electronic drum loop\", steps=50, cfg_scale=7, return_latents=False)\n",
    "#     # play audio\n",
    "#     ipd.display(ipd.Audio(audio.cpu().numpy()[0], rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - Inpainting Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AND ENCODE REFERENCE AUDIO\n",
    "def load_and_encode_audio(path, model):\n",
    "    audio, sr = torchaudio.load(path)\n",
    "    # resample to SAMPLE_RATE\n",
    "    resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
    "    sr = SAMPLE_RATE\n",
    "    audio = resampler(audio)\n",
    "    # peak normalize\n",
    "    audio = audio / audio.abs().max()\n",
    "\n",
    "    # trim to SAMPLE_SIZE if longer, pad with repetition if shorter\n",
    "    if audio.shape[1] < SAMPLE_SIZE:\n",
    "        while audio.shape[1] < SAMPLE_SIZE:\n",
    "            audio = torch.cat((audio, audio), dim=1)\n",
    "\n",
    "    audio = audio[:, :SAMPLE_SIZE][None].to(device)\n",
    "\n",
    "    reference = model.pretransform.encode(audio)\n",
    "    return reference\n",
    "\n",
    "\n",
    "def load_encoded_audio(path):\n",
    "    encoded_latent = torch.load(path)\n",
    "    # check if the latent is in half precision\n",
    "    return encoded_latent.half().to(device)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inpainting_mask(reference, mask_start_s, mask_end_s):\n",
    "    # TODO\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test your function, we provide some exampple latents to compare against\n",
    "# the MSE between your latents and the reference latents should be low\n",
    "for ix in range(2):\n",
    "    for midx, mask_range in enumerate([(0,4), (1,2), (1.5,3), (2,4), (1,4), (0,5)]):\n",
    "        # load reference from testing_files\n",
    "        ref = torch.load(f\"testing_files/q2_{ix}.pt\")[midx].to(device)\n",
    "        # load reference audio\n",
    "        reference = load_encoded_audio(f\"testing_files/q1_{ix}.pt\")\n",
    "        # generate mask\n",
    "        mask = generate_inpainting_mask(reference, *mask_range)\n",
    "        # compare latents\n",
    "        print(f\"Latent MSE: {torch.nn.functional.mse_loss(ref, mask)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3  - Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simple_sample_inpaint(model, x, sigmas, reference, mask, extra_args=None):\n",
    "    extra_args = {} if extra_args is None else extra_args\n",
    "    s_in = x.new_ones([x.shape[0]])\n",
    "    for i in trange(len(sigmas) - 1):\n",
    "        # TODO\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint(prompt=\"128 BPM house drum loop\", steps=50, cfg_scale=7, reference=None, mask_start_s=20, mask_end_s=30, return_latents=False, x_start=None):\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 5\n",
    "    }]\n",
    "    # Set up inpainting mask\n",
    "    mask = generate_inpainting_mask(reference, mask_start_s, mask_end_s)\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    if x_start is not None:\n",
    "        x_T = x_start\n",
    "\n",
    "    # Sample\n",
    "    inp_samples = simple_sample_inpaint(denoiser, x_T, sigmas, reference, mask, extra_args=extra_args)\n",
    "    del x_T\n",
    "    del sigmas\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "\n",
    "    # decode and play\n",
    "    inpainted_audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        inp_samples\n",
    "    ).cpu()\n",
    "    return inpainted_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test your function, we provide some exampple latents to compare against\n",
    "# the MSE between your latents and the reference latents should be low\n",
    "\n",
    "for ix, prompt in enumerate([\"lo-fi jazz piano in a rainy cafe\", \"deep ambient wash with ocean sounds\"]):\n",
    "    # load reference from testing_files\n",
    "    ref = torch.load(f\"testing_files/q3_{ix}.pt\").to(device)\n",
    "    # load reference audio\n",
    "    reference = load_encoded_audio(f\"testing_files/q1_{ix}.pt\")\n",
    "    # generate mask\n",
    "    mask = generate_inpainting_mask(reference, 0, 3)\n",
    "\n",
    "    # load x_T\n",
    "    x_T = torch.load(f\"x_T.pt\").to(device)\n",
    "\n",
    "    # generate inpainting\n",
    "    latents = inpaint(prompt=prompt, steps=50, cfg_scale=7, reference=reference, mask_start_s=0, mask_end_s=3, return_latents=True, x_start=x_T)\n",
    "    # compare latents\n",
    "    print(f\"Latent MSE: {torch.nn.functional.mse_loss(ref, latents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those running with a GPU, you can also generate audio samples\n",
    "# this is unpractical for CPU, but you can try it if you want (we DO NOT recommend it)\n",
    "# for ix, prompt in enumerate([\"lo-fi jazz piano in a rainy cafe\", \"deep ambient wash with ocean sounds\"]):\n",
    "#     # load reference from testing_files\n",
    "#     reference = load_encoded_audio(f\"testing_files/q1_{ix}.pt\")\n",
    "#     # generate mask\n",
    "#     mask = generate_inpainting_mask(reference, 0, 3)\n",
    "#     # generate inpainting\n",
    "#     audio = inpaint(prompt=prompt, steps=50, cfg_scale=7, reference=reference, mask_start_s=0, mask_end_s=3, return_latents=False)\n",
    "#     # play audio\n",
    "#     ipd.display(ipd.Audio(audio.cpu().numpy()[0], rate=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Painting with Starting and Stopping Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simple_sample_variable_inpaint(model, x, sigmas, reference, mask, extra_args=None, paint_start=None, paint_end=None):\n",
    "    if paint_start is None:\n",
    "        paint_start = 0\n",
    "    if paint_end is None:\n",
    "        paint_end = len(sigmas) - 1\n",
    "    extra_args = {} if extra_args is None else extra_args\n",
    "    s_in = x.new_ones([x.shape[0]])\n",
    "    for i in trange(len(sigmas) - 1):\n",
    "        # TODO\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_inpaint(prompt=\"128 BPM house drum loop\", steps=50, cfg_scale=7, reference=None, mask_start_s=20, mask_end_s=30, paint_start=None, paint_end=None, return_latents=False, x_start=None):\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 5\n",
    "    }]\n",
    "    # Set up inpainting mask\n",
    "    mask = generate_inpainting_mask(reference, mask_start_s, mask_end_s)\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    if x_start is not None:\n",
    "        x_T = x_start\n",
    "\n",
    "    # Sample\n",
    "    inp_samples = simple_sample_variable_inpaint(denoiser, x_T, sigmas, reference, mask, extra_args=extra_args, paint_start=paint_start, paint_end=paint_end)\n",
    "    del x_T\n",
    "    del sigmas\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "\n",
    "    # decode and play\n",
    "    inpainted_audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        inp_samples\n",
    "    ).cpu()\n",
    "    return inpainted_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test your function, we provide some exampple latents to compare against\n",
    "# the MSE between your latents and the reference latents should be low\n",
    "\n",
    "for ix, prompt in enumerate([\"lo-fi jazz piano in a rainy cafe\", \"deep ambient wash with ocean sounds\"]):\n",
    "    # load reference from testing_files\n",
    "    ref = torch.load(f\"testing_files/q4_{ix}.pt\").to(device)\n",
    "\n",
    "    # load reference audio\n",
    "    reference = load_encoded_audio(f\"testing_files/q1_{ix}.pt\")\n",
    "\n",
    "    if ix == 0:\n",
    "        mask = generate_inpainting_mask(reference, 0, 3)\n",
    "        paint_start = 0\n",
    "        paint_end = 20\n",
    "    else:\n",
    "        mask = generate_inpainting_mask(reference, 2, 5)\n",
    "        paint_start = 15\n",
    "        paint_end = 45\n",
    "\n",
    "    # load x_T\n",
    "    x_T = torch.load(f\"x_T.pt\").to(device)\n",
    "\n",
    "    # generate inpainting\n",
    "    latents = variable_inpaint(prompt=prompt, steps=50, cfg_scale=7, reference=reference, mask_start_s=0, mask_end_s=3, paint_start=paint_start, paint_end=paint_end, return_latents=True, x_start=x_T)\n",
    "    # compare latents\n",
    "    print(f\"Latent MSE: {torch.nn.functional.mse_loss(ref, latents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those running with a GPU, you can also generate audio samples\n",
    "# this is unpractical for CPU, but you can try it if you want (we DO NOT recommend it)\n",
    "# for ix, prompt in enumerate([\"lo-fi jazz piano in a rainy cafe\", \"deep ambient wash with ocean sounds\"]):\n",
    "#     # load reference from testing_files\n",
    "#     reference = load_encoded_audio(f\"testing_files/q1_{ix}.pt\")\n",
    "#     # generate mask\n",
    "#     mask = generate_inpainting_mask(reference, 0, 3)\n",
    "#     # generate inpainting\n",
    "#     if ix == 0:\n",
    "#         paint_start = 0\n",
    "#         paint_end = 20\n",
    "#     else: \n",
    "#         paint_start = 15\n",
    "#         paint_end = 45    \n",
    "#     audio = variable_inpaint(prompt=prompt, steps=50, cfg_scale=7, reference=reference, mask_start_s=0, mask_end_s=3, paint_start=paint_start, paint_end=paint_end, return_latents=False)\n",
    "#     # play audio\n",
    "#     ipd.display(ipd.Audio(audio.cpu().numpy()[0], rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sample_style_transfer(model, sigmas, reference, extra_args=None, transfer_strength=0):\n",
    "    # TODO\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(prompt=\"128 BPM house drum loop\", steps=50, cfg_scale=7, reference=None, transfer_strength=0, return_latents=False, x_start=None):\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 5\n",
    "    }]\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "    if x_start is not None:\n",
    "        x_T = x_start\n",
    "\n",
    "    # Sample\n",
    "    inp_samples = simple_sample_style_transfer(denoiser, sigmas, reference, extra_args=extra_args, transfer_strength=transfer_strength)\n",
    "    del x_T\n",
    "    del sigmas\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "    \n",
    "    # decode and play\n",
    "    inpainted_audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        inp_samples\n",
    "    ).cpu()\n",
    "    return inpainted_audio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test your function, we provide some exampple latents to compare against\n",
    "# the MSE between your latents and the reference latents should be low\n",
    "for ix, prompt in enumerate([\"deep ambient wash with ocean sounds\", \"lo-fi jazz piano in a rainy cafe\"]):\n",
    "    # load reference from testing_files\n",
    "    ref = torch.load(f\"testing_files/q5_{ix}.pt\").to(device)\n",
    "    # load reference audio\n",
    "    reference = load_encoded_audio(f\"testing_files/q1_{ix}.pt\")\n",
    "    if ix == 0:\n",
    "        transfer_strength = 0.2\n",
    "    else:\n",
    "        transfer_strength = 0.5\n",
    "\n",
    "    # load x_T\n",
    "    x_T = torch.load(f\"x_T.pt\").to(device)\n",
    "\n",
    "    # generate style transfer\n",
    "    latents = style_transfer(prompt=prompt, steps=50, cfg_scale=7, reference=reference, transfer_strength=transfer_strength, return_latents=True, x_start=x_T)\n",
    "    # compare latents\n",
    "    print(f\"Latent MSE: {torch.nn.functional.mse_loss(ref, latents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those running with a GPU, you can also generate audio samples\n",
    "# this is unpractical for CPU, but you can try it if you want (we DO NOT recommend it)\n",
    "# for ix, prompt in enumerate([\"lo-fi jazz piano in a rainy cafe\", \"deep ambient wash with ocean sounds\"]):\n",
    "#     # load reference from testing_files\n",
    "#     reference = load_encoded_audio(f\"testing_files/q1_{ix}.pt\")\n",
    "#     if ix == 0:\n",
    "#         transfer_strength = 0.2\n",
    "#     else:\n",
    "#         transfer_strength = 0.5\n",
    "#     # generate style transfer\n",
    "#     audio = style_transfer(prompt=prompt, steps=50, cfg_scale=7, reference=reference, transfer_strength=transfer_strength, return_latents=False)\n",
    "#     # play audio\n",
    "#     ipd.display(ipd.Audio(audio.cpu().numpy()[0], rate=SAMPLE_RATE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
